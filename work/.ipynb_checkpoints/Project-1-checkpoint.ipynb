{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77e3428-fa17-440a-b8f3-41470c70fda0",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "### Group J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be46f09-5c4a-40a3-a400-9a75737c80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import json\n",
    "\n",
    "spark_ok = True\n",
    "try:\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.context import SparkContext\n",
    "except Exception as e:\n",
    "    spark_ok = False\n",
    "    print(\"PySpark not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3e7331-a80d-4728-8eb1-505373962d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.0\n"
     ]
    }
   ],
   "source": [
    "# Init Spark\n",
    "\n",
    "sc = SparkContext('local', 'project_1')\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('project_1')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e721dc2d-74ff-4859-853a-d327e23dfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables, helpers\n",
    "\n",
    "INBOX_DIR = \"data/inbox\"\n",
    "STATE_DIR = \"state\"\n",
    "MANIFEST_PATH = os.path.join(STATE_DIR, \"manifest.json\")\n",
    "LOOKUP_TABLE = \"taxi_zone_lookup.parquet\"\n",
    "\n",
    "OUTBOX_DIR = \"data/outbox\"\n",
    "OUTPUT_PATH = os.path.join(OUTBOX_DIR, \"trips_enriched.parquet\")\n",
    "\n",
    "\n",
    "def load_manifest(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"processed_files\": []}\n",
    "\n",
    "\n",
    "def save_manifest(path, manifest_obj):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest_obj, f, indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "\n",
    "def list_parquet_files(inbox_dir):\n",
    "    if not os.path.isdir(inbox_dir):\n",
    "        return []\n",
    "    return sorted(\n",
    "        os.path.join(inbox_dir, fn)\n",
    "        for fn in os.listdir(inbox_dir)\n",
    "        if fn.lower().endswith(\".parquet\")\n",
    "    )\n",
    "\n",
    "\n",
    "def file_size_bytes(path):\n",
    "    try:\n",
    "        return os.path.getsize(path)\n",
    "    except OSError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_existing_output(spark, output_path):\n",
    "    if os.path.exists(output_path):\n",
    "        return spark.read.parquet(output_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e75cf18-0381-4daa-819d-6ad56e721f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parquet files in inbox: 3\n",
      "Trip candidate files: 2\n",
      "New trip files to process: 2\n",
      "  NEW: yellow_tripdata_2025-01.parquet\n",
      "  NEW: yellow_tripdata_2025-02.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "manifest = load_manifest(MANIFEST_PATH)\n",
    "already_processed = {x[\"file\"] for x in manifest.get(\"processed_files\", [])}\n",
    "\n",
    "inbox_files = list_parquet_files(INBOX_DIR)\n",
    "\n",
    "# Ignore lookup table file if its already in inbox\n",
    "candidate_trip_files = [\n",
    "    p for p in inbox_files\n",
    "    if os.path.basename(p) != LOOKUP_TABLE\n",
    "]\n",
    "\n",
    "new_trip_files = [\n",
    "    p for p in candidate_trip_files\n",
    "    if os.path.basename(p) not in already_processed\n",
    "]\n",
    "\n",
    "print(f\"Total parquet files in inbox: {len(inbox_files)}\")\n",
    "print(f\"Trip candidate files: {len(candidate_trip_files)}\")\n",
    "print(f\"New trip files to process: {len(new_trip_files)}\")\n",
    "for p in new_trip_files:\n",
    "    print(\"  NEW:\", os.path.basename(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "024c3124-efc1-48d5-af5a-d4c599894338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "+----------+-------------+-----------------------+------------+\n",
      "|LocationID|Borough      |Zone                   |service_zone|\n",
      "+----------+-------------+-----------------------+------------+\n",
      "|1         |EWR          |Newark Airport         |EWR         |\n",
      "|2         |Queens       |Jamaica Bay            |Boro Zone   |\n",
      "|3         |Bronx        |Allerton/Pelham Gardens|Boro Zone   |\n",
      "|4         |Manhattan    |Alphabet City          |Yellow Zone |\n",
      "|5         |Staten Island|Arden Heights          |Boro Zone   |\n",
      "+----------+-------------+-----------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load zone lookup table\n",
    "\n",
    "LOOKUP_PATH = os.path.join(INBOX_DIR, LOOKUP_TABLE)\n",
    "\n",
    "lookup_df = (\n",
    "    spark.read.parquet(LOOKUP_PATH)\n",
    "    .select(\n",
    "        F.col(\"LocationID\").cast(\"int\").alias(\"LocationID\"),\n",
    "        F.col(\"Borough\").alias(\"Borough\"),\n",
    "        F.col(\"Zone\").alias(\"Zone\"),\n",
    "        F.col(\"service_zone\").alias(\"service_zone\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "lookup_df.printSchema()\n",
    "lookup_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ab19716-fdf0-4f54-8176-71214f1ea0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum transformations\n",
    "\n",
    "def normalize_trip_schema(df):\n",
    "    \"\"\"\n",
    "    Normalizes both Yellow (tpep_*) and Green (lpep_*) taxi schemas to:\n",
    "      pickup_ts, dropoff_ts, pickup_location_id, dropoff_location_id,\n",
    "      passenger_count, trip_distance\n",
    "    \"\"\"\n",
    "    if \"tpep_pickup_datetime\" in df.columns:\n",
    "        pickup_col = \"tpep_pickup_datetime\"\n",
    "        dropoff_col = \"tpep_dropoff_datetime\"\n",
    "    elif \"lpep_pickup_datetime\" in df.columns:\n",
    "        pickup_col = \"lpep_pickup_datetime\"\n",
    "        dropoff_col = \"lpep_dropoff_datetime\"\n",
    "    else:\n",
    "        raise ValueError(\"Could not find pickup/dropoff datetime columns (tpep_* or lpep_*)\")\n",
    "\n",
    "    df = df.withColumnRenamed(pickup_col, \"pickup_ts\").withColumnRenamed(dropoff_col, \"dropoff_ts\")\n",
    "\n",
    "    if \"PULocationID\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"PULocationID\", \"pickup_location_id\")\n",
    "    if \"DOLocationID\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"DOLocationID\", \"dropoff_location_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_minimum(raw_df):\n",
    "    \"\"\"\n",
    "    Minimum required transformations:\n",
    "      1) parse/cast types\n",
    "      2) clean invalids/nulls (rules documented below)\n",
    "      3) deduplicate by defined key\n",
    "    \"\"\"\n",
    "    df = normalize_trip_schema(raw_df)\n",
    "\n",
    "    # Cast/parse types\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_ts\"))\n",
    "        .withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_ts\"))\n",
    "        .withColumn(\"pickup_location_id\", F.col(\"pickup_location_id\").cast(\"int\"))\n",
    "        .withColumn(\"dropoff_location_id\", F.col(\"dropoff_location_id\").cast(\"int\"))\n",
    "        .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(\"int\"))\n",
    "        .withColumn(\"trip_distance\", F.col(\"trip_distance\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "    # Cleaning rules:\n",
    "    # - Remove rows with missing timestamps\n",
    "    # - Remove rows where dropoff <= pickup\n",
    "    df = df.filter(F.col(\"pickup_ts\").isNotNull() & F.col(\"dropoff_ts\").isNotNull())\n",
    "    df = df.filter(F.col(\"dropoff_ts\") > F.col(\"pickup_ts\"))\n",
    "\n",
    "    # - Remove rows with missing or non-positive location IDs\n",
    "    df = df.filter(F.col(\"pickup_location_id\").isNotNull() & (F.col(\"pickup_location_id\") > 0))\n",
    "    df = df.filter(F.col(\"dropoff_location_id\").isNotNull() & (F.col(\"dropoff_location_id\") > 0))\n",
    "\n",
    "    # - Passenger count: null -> 0, then remove negative\n",
    "    df = df.withColumn(\"passenger_count\", F.coalesce(F.col(\"passenger_count\"), F.lit(0)))\n",
    "    df = df.filter(F.col(\"passenger_count\") >= 0)\n",
    "\n",
    "    # - Trip distance: null -> 0.0, then remove <= 0\n",
    "    df = df.withColumn(\"trip_distance\", F.coalesce(F.col(\"trip_distance\"), F.lit(0.0)))\n",
    "    df = df.filter(F.col(\"trip_distance\") > 0.0)\n",
    "\n",
    "    # ---- Deduplication ----\n",
    "    dedup_key = [\n",
    "        \"source_file\",\n",
    "        \"pickup_ts\", \"dropoff_ts\",\n",
    "        \"pickup_location_id\", \"dropoff_location_id\",\n",
    "        \"passenger_count\", \"trip_distance\",\n",
    "    ]\n",
    "    df = df.dropDuplicates(dedup_key)\n",
    "\n",
    "    # Derived fields\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"trip_duration_minutes\",\n",
    "            (F.col(\"dropoff_ts\").cast(\"long\") - F.col(\"pickup_ts\").cast(\"long\")) / F.lit(60.0)\n",
    "        )\n",
    "        .withColumn(\"pickup_date\", F.to_date(\"pickup_ts\"))\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41e50f07-6984-43a1-ae89-d095e73f82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichment functions\n",
    "\n",
    "def enrich_with_zones(df, lookup_df):\n",
    "    lk = lookup_df.select(\"LocationID\", F.col(\"Zone\").alias(\"zone_name\"))\n",
    "\n",
    "    pu = lk.select(\n",
    "        F.col(\"LocationID\").alias(\"pickup_location_id\"),\n",
    "        F.col(\"zone_name\").alias(\"pickup_zone_name\")\n",
    "    )\n",
    "    do = lk.select(\n",
    "        F.col(\"LocationID\").alias(\"dropoff_location_id\"),\n",
    "        F.col(\"zone_name\").alias(\"dropoff_zone_name\")\n",
    "    )\n",
    "\n",
    "    # broadcast is a good optimization for small dimension tables\n",
    "    df = df.join(F.broadcast(pu), on=\"pickup_location_id\", how=\"left\")\n",
    "    df = df.join(F.broadcast(do), on=\"dropoff_location_id\", how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_required_fields(df):\n",
    "    return df.select(\n",
    "        \"pickup_ts\", \"dropoff_ts\",\n",
    "        \"pickup_location_id\", \"dropoff_location_id\",\n",
    "        \"pickup_zone_name\", \"dropoff_zone_name\",\n",
    "        \"passenger_count\", \"trip_distance\",\n",
    "        \"trip_duration_minutes\", \"pickup_date\",\n",
    "        \"source_file\", \"ingested_at\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e3d8136-2bc7-49f7-a514-bd31dd83e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctness helper for sampling bad rows before cleaning\n",
    "\n",
    "def find_bad_rows_examples(raw_df, limit=3):\n",
    "    df = normalize_trip_schema(raw_df)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_ts\"))\n",
    "        .withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_ts\"))\n",
    "        .withColumn(\"pickup_location_id\", F.col(\"pickup_location_id\").cast(\"int\"))\n",
    "        .withColumn(\"dropoff_location_id\", F.col(\"dropoff_location_id\").cast(\"int\"))\n",
    "        .withColumn(\"passenger_count\", F.col(\"passenger_count\").cast(\"int\"))\n",
    "        .withColumn(\"trip_distance\", F.col(\"trip_distance\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "    bad_conditions = (\n",
    "        F.col(\"pickup_ts\").isNull()\n",
    "        | F.col(\"dropoff_ts\").isNull()\n",
    "        | (F.col(\"dropoff_ts\") <= F.col(\"pickup_ts\"))\n",
    "        | F.col(\"pickup_location_id\").isNull() | (F.col(\"pickup_location_id\") <= 0)\n",
    "        | F.col(\"dropoff_location_id\").isNull() | (F.col(\"dropoff_location_id\") <= 0)\n",
    "        | F.col(\"passenger_count\").isNull() | (F.col(\"passenger_count\") < 0)\n",
    "        | F.col(\"trip_distance\").isNull() | (F.col(\"trip_distance\") <= 0)\n",
    "    )\n",
    "\n",
    "    return df.filter(bad_conditions).select(\n",
    "        \"pickup_ts\", \"dropoff_ts\",\n",
    "        \"pickup_location_id\", \"dropoff_location_id\",\n",
    "        \"passenger_count\", \"trip_distance\",\n",
    "        \"source_file\"\n",
    "    ).limit(limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07f8e399-df31-47c4-b621-a70ea1a15cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New raw schema:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      " |-- source_file: string (nullable = false)\n",
      " |-- ingested_at: timestamp (nullable = false)\n",
      "\n",
      "\n",
      "Input rows (new files): 7052769\n",
      "\n",
      "Bad row examples (up to 3):\n",
      "Row(pickup_ts=datetime.datetime(2025, 2, 1, 0, 30, 36), dropoff_ts=datetime.datetime(2025, 2, 1, 0, 31, 28), pickup_location_id=4, dropoff_location_id=4, passenger_count=1, trip_distance=0.0, source_file='yellow_tripdata_2025-02.parquet')\n",
      "Row(pickup_ts=datetime.datetime(2025, 2, 1, 0, 1, 10), dropoff_ts=datetime.datetime(2025, 2, 1, 0, 1, 10), pickup_location_id=161, dropoff_location_id=141, passenger_count=1, trip_distance=1.14, source_file='yellow_tripdata_2025-02.parquet')\n",
      "Row(pickup_ts=datetime.datetime(2025, 2, 1, 0, 32, 48), dropoff_ts=datetime.datetime(2025, 2, 1, 0, 32, 48), pickup_location_id=230, dropoff_location_id=142, passenger_count=2, trip_distance=1.4, source_file='yellow_tripdata_2025-02.parquet')\n",
      "\n",
      "Rows after cleaning + dedup (new batch): 6762459\n",
      "\n",
      "Previous output rows: 6762459\n",
      "Final output rows after merge: 6762459\n",
      "\n",
      "Wrote output dataset to: data/outbox/trips_enriched.parquet\n",
      "Updated manifest: state/manifest.json\n"
     ]
    }
   ],
   "source": [
    "# INCREMENTAL RUN JOB\n",
    "# read -> clean -> enrich -> merge -> write -> manifest\n",
    "\n",
    "if not new_trip_files:\n",
    "    print(\"No new files to process. Job is idempotent; nothing to do.\")\n",
    "else:\n",
    "    # Read new data\n",
    "    new_raw_df = (\n",
    "        spark.read.parquet(*new_trip_files)\n",
    "        .withColumn(\"source_file\", F.regexp_extract(F.input_file_name(), r\"([^/\\\\\\\\]+)$\", 1))\n",
    "        .withColumn(\"ingested_at\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    print(\"\\nNew raw schema:\")\n",
    "    new_raw_df.printSchema()\n",
    "\n",
    "    # Correctness evidence\n",
    "    input_rows = new_raw_df.count()\n",
    "    print(\"\\nInput rows (new files):\", input_rows)\n",
    "\n",
    "    print(\"\\nBad row examples (up to 3):\")  # Bad row examples\n",
    "    for r in find_bad_rows_examples(new_raw_df, limit=3).collect():\n",
    "        print(r)\n",
    "\n",
    "    # Transform\n",
    "    new_clean_df = transform_minimum(new_raw_df).cache()\n",
    "    after_clean_rows = new_clean_df.count()\n",
    "    print(\"\\nRows after cleaning + dedup (new batch):\", after_clean_rows)\n",
    "\n",
    "    # Enrich\n",
    "    new_enriched_df = enrich_with_zones(new_clean_df, lookup_df)\n",
    "    new_enriched_df = select_required_fields(new_enriched_df)\n",
    "\n",
    "    # Merge with previous output\n",
    "    existing_df = read_existing_output(spark, OUTPUT_PATH)\n",
    "    if existing_df is None:\n",
    "        prev_rows = 0\n",
    "        merged_df = new_enriched_df\n",
    "    else:\n",
    "        prev_rows = existing_df.count()\n",
    "        merged_df = existing_df.unionByName(new_enriched_df, allowMissingColumns=True)\n",
    "\n",
    "    # Global dedup\n",
    "    global_dedup_key = [\n",
    "        \"source_file\",\n",
    "        \"pickup_ts\", \"dropoff_ts\",\n",
    "        \"pickup_location_id\", \"dropoff_location_id\",\n",
    "        \"passenger_count\", \"trip_distance\",\n",
    "    ]\n",
    "    final_df = merged_df.dropDuplicates(global_dedup_key)\n",
    "\n",
    "    final_rows = final_df.count()\n",
    "    print(\"\\nPrevious output rows:\", prev_rows)\n",
    "    print(\"Final output rows after merge:\", final_rows)\n",
    "\n",
    "    # Write output\n",
    "    os.makedirs(OUTBOX_DIR, exist_ok=True)\n",
    "    (\n",
    "        final_df\n",
    "        .coalesce(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(OUTPUT_PATH)\n",
    "    )\n",
    "    print(\"\\nWrote output dataset to:\", OUTPUT_PATH)\n",
    "\n",
    "    # Update manifest\n",
    "    processed_records = manifest.get(\"processed_files\", [])\n",
    "    ingested_time_str = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    for p in new_trip_files:\n",
    "        processed_records.append({\n",
    "            \"file\": os.path.basename(p),\n",
    "            \"size_bytes\": file_size_bytes(p),\n",
    "            \"ingested_utc\": ingested_time_str,\n",
    "        })\n",
    "\n",
    "    manifest[\"processed_files\"] = processed_records\n",
    "    save_manifest(MANIFEST_PATH, manifest)\n",
    "    print(\"Updated manifest:\", MANIFEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda932f4-4265-455e-96eb-356ba5559f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
